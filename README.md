# BrainMRIModel

Bu proje, beyin MRI gÃ¶rÃ¼ntÃ¼lerinden tÃ¼mÃ¶r tespiti ve sÄ±nÄ±flandÄ±rmasÄ± iÃ§in derin Ã¶ÄŸrenme tabanlÄ± bir model geliÅŸtirmeyi amaÃ§lamaktadÄ±r. Proje, PyTorch Ã§erÃ§evesini kullanarak Ã¶zel bir EvriÅŸimsel Sinir AÄŸÄ± (CNN) mimarisi eÄŸitir ve model performansÄ±nÄ± artÄ±rmak iÃ§in Ã§eÅŸitli geliÅŸmiÅŸ eÄŸitim stratejileri ve veri artÄ±rma teknikleri kullanÄ±r.

## KullanÄ±lan Teknolojiler ve KÃ¼tÃ¼phaneler

*   **Derin Ã–ÄŸrenme Ã‡erÃ§evesi:** PyTorch
*   **Veri Ä°ÅŸleme:** NumPy, PIL (Python Imaging Library), Albumentations
*   **GÃ¶rselleÅŸtirme:** Matplotlib
*   **Optimizasyon:** `torch.optim`
*   **GeliÅŸtirme Dili:** Python

## Model Mimarisi (models/basic_cnn_model.py)

Projenin kalbinde, Ã¶zel olarak tasarlanmÄ±ÅŸ `BasicCNNModel` adÄ±nda VGG benzeri bir EvriÅŸimsel Sinir AÄŸÄ± (CNN) bulunmaktadÄ±r.

*   **Temel Katmanlar:** `nn.Conv2d`, `nn.GroupNorm`, `nn.SiLU`, `nn.Identity`, `nn.Dropout2d`, `nn.AdaptiveAvgPool2d`, `nn.BatchNorm1d`, `nn.Mish`, `nn.Linear`.
*   **`TrainOnlyNoise`:** Sadece eÄŸitim sÄ±rasÄ±nda girdi verisine gÃ¼rÃ¼ltÃ¼ ekleyen Ã¶zel bir katman.
*   **`ConvBlock`:** Her biri evriÅŸim, 32 gruplu grup normalizasyonu ve `SiLU` aktivasyonu iÃ§eren temel yapÄ± taÅŸlarÄ±. Bu bloklar, bir `Dropout2d` ve `TrainOnlyNoise` katmanÄ± da iÃ§erir. KalÄ±ntÄ± baÄŸlantÄ±lar (shortcut connections) bu blok iÃ§inde uygulanmamÄ±ÅŸtÄ±r.
*   **Blok YapÄ±sÄ±:** Model, ayrÄ± ayrÄ± dondurulabilen ve aÃ§Ä±labilen Ã¼Ã§ ana bloktan oluÅŸur. Bu, aÅŸamalÄ± Ã¶ÄŸrenme stratejisini mÃ¼mkÃ¼n kÄ±lar.
*   **Dinamik AdaptÃ¶rler:** Son aÃ§Ä±lan bloÄŸun Ã§Ä±ktÄ±sÄ±nÄ± sÄ±nÄ±flandÄ±rma katmanÄ±na uygun hale getiren adaptÃ¶r katmanlarÄ±.
*   **AÄŸÄ±rlÄ±k BaÅŸlatma:** Kaiming Normal (He normal) ve sabit baÅŸlatma teknikleri kullanÄ±lmÄ±ÅŸtÄ±r.
*   **AÅŸamalÄ± Ã–ÄŸrenme (Progressive Learning):** `freeze_blocks_until` metodu ile modelin katmanlarÄ± kademeli olarak eÄŸitilir. BaÅŸlangÄ±Ã§ta yalnÄ±zca ilk blok (Blok 1) eÄŸitilebilir durumdadÄ±r. EÄŸitim sÄ±rasÄ±nda, belirli koÅŸullar altÄ±nda (ÅŸu anki uygulamada Blok 2, 15. epokta aÃ§Ä±lmaya zorlanÄ±r; Blok 3'Ã¼n aÃ§Ä±lmasÄ± ise engellenir) diÄŸer bloklar dondurulmuÅŸ durumdan Ã§Ä±karÄ±lÄ±r.

    *   **Blok DetaylarÄ±:**
        *   **Blok 1:** GiriÅŸ katmanlarÄ±nÄ± (Ã¶rn. 3 kanal) alÄ±r ve 64 Ã§Ä±kÄ±ÅŸ kanalÄ± Ã¼retir. Ä°ki adet `ConvBlock` katmanÄ± ve bir `Dropout2d` katmanÄ± iÃ§erir. Ä°kinci `ConvBlock` downsampling (`stride=2`) yapar.
        *   **Blok 2:** 64 giriÅŸ kanalÄ± ile baÅŸlar ve 512 Ã§Ä±kÄ±ÅŸ kanalÄ± Ã¼retir. ÃœÃ§ adet `ConvBlock` katmanÄ± ve bir `Dropout2d` katmanÄ± iÃ§erir. Son `ConvBlock` downsampling (`stride=2`) yapar ve `dilation=2` kullanÄ±r.
        *   **Blok 3:** 512 giriÅŸ kanalÄ± ile baÅŸlar ve 256 Ã§Ä±kÄ±ÅŸ kanalÄ± Ã¼retir. ÃœÃ§ adet `ConvBlock` katmanÄ± ve bir `Dropout2d` katmanÄ± iÃ§erir. Son `ConvBlock` downsampling (`stride=2`) yapar.
*   **Ã‡Ä±kÄ±ÅŸ Dense KatmanlarÄ±:**
    *   **Blok 1 Ã‡Ä±kÄ±ÅŸÄ±:** 64 kanal Ã§Ä±kÄ±ÅŸÄ±, 128 nÃ¶ronlu bir dense katmanÄ±na baÄŸlanÄ±r.
    *   **Blok 2 Ã‡Ä±kÄ±ÅŸÄ±:** 512 kanal Ã§Ä±kÄ±ÅŸÄ± doÄŸrudan son dense katmanÄ±na baÄŸlanÄ±r.
    *   **Blok 3 Ã‡Ä±kÄ±ÅŸÄ±:** 256 kanal Ã§Ä±kÄ±ÅŸÄ±, 128 nÃ¶ronlu bir dense katmanÄ±na baÄŸlanÄ±r.
    *   **Son Dense KatmanÄ±:** TÃ¼m bloklarÄ±n dense katmanlarÄ± birleÅŸtirilerek 512 nÃ¶ronlu bir ara katmana baÄŸlanÄ±r ve son olarak 3 sÄ±nÄ±flÄ± Ã§Ä±kÄ±ÅŸ katmanÄ±na (softmax) baÄŸlanÄ±r.

## Blok KullanÄ±mÄ± ve Katman SayÄ±sÄ±

Modelin eÄŸitimi, hangi blokta sonlandÄ±ÄŸÄ±na baÄŸlÄ± olarak farklÄ± sayÄ±da katman kullanÄ±r:

*   **Blok 1 ile SonlanÄ±rsa:**
    *   2 adet `ConvBlock` (her biri 4 katman: Conv2d, GroupNorm, SiLU, Dropout2d)
    *   1 adet Dropout2d (0.1)
    *   1 adet AdaptiveAvgPool2d
    *   1 adet BatchNorm1d
    *   1 adet Linear (64->512) adaptÃ¶r
    *   1 adet Linear (512->3) sÄ±nÄ±flandÄ±rÄ±cÄ±
    *   Toplam: 2 adet conv2d - 2 adet dense

*   **Blok 2 ile SonlanÄ±rsa:**
    *   Blok 1'in tÃ¼m katmanlarÄ±
    *   3 adet `ConvBlock` (her biri 4 katman: Conv2d, GroupNorm, SiLU, Dropout2d)
    *   1 adet Dropout2d (0.2)
    *   1 adet BatchNorm1d
    *   1 adet Identity adaptÃ¶r (512->512)
    *   1 adet Linear (512->3) sÄ±nÄ±flandÄ±rÄ±cÄ±
    *   Toplam: ~ 2 + 2 adet conv2d 1 dense 

*   **Blok 3 ile SonlanÄ±rsa: (AÃ§Ä±lmasÄ± ÅŸuanki TasarÄ±m ile izin verilmiyor)**
    *   Blok 1 ve 2'nin tÃ¼m katmanlarÄ±
    *   3 adet `ConvBlock` (her biri 4 katman: Conv2d, GroupNorm, SiLU, Dropout2d)
    *   1 adet Dropout2d (0.3)
    *   1 adet AdaptiveAvgPool2d
    *   1 adet BatchNorm1d
    *   1 adet Linear (256->512) adaptÃ¶r
    *   1 adet Linear (512->3) sÄ±nÄ±flandÄ±rÄ±cÄ±
    *   Toplam: ~2 + 2 + 3 adet conv2d 1 adet pooling 2 adet dense

Not: Her `ConvBlock` iÃ§indeki shortcut connection'lar da hesaba katÄ±ldÄ±ÄŸÄ±nda, gerÃ§ek katman sayÄ±sÄ± biraz daha yÃ¼ksek olabilir. AyrÄ±ca, eÄŸitim sÄ±rasÄ±nda bazÄ± katmanlar dondurulmuÅŸ (frozen) olabilir, bu durumda aktif katman sayÄ±sÄ± azalÄ±r.

## EÄŸitim ProsedÃ¼rleri ve AlgoritmalarÄ± (models/train.py)

Modelin eÄŸitimi iÃ§in Ã§eÅŸitli modern teknikler ve algoritmalar kullanÄ±lmÄ±ÅŸtÄ±r:

*   **Optimizasyon:** `torch.optim.AdamW`.
*   **KayÄ±p Fonksiyonu:** `nn.CrossEntropyLoss` (sÄ±nÄ±f dengesizliÄŸini gidermek iÃ§in sÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ± ile).
*   **Ã–ÄŸrenme OranÄ± Ã‡izelgeleyiciler (LR Schedulers):**
    *   **BaÅŸlangÄ±Ã§ LR:** `1e-2` ile baÅŸlayan ve aÅŸamalÄ± olarak deÄŸiÅŸen Ã¶ÄŸrenme oranÄ± stratejisi.
    *   **IsÄ±nma FazÄ± (Warmup):** Ä°lk 5 epokta lineer artÄ±ÅŸ ile `1e-2`'ye ulaÅŸÄ±r.
    *   **Ana EÄŸitim FazÄ±:**
        *   `OneCycleLR`: 5-20. epoklar arasÄ±nda maksimum LR'ye (`1e-2`) ulaÅŸÄ±r ve sonra azalÄ±r.
        *   `CosineAnnealingLR`: 20-40. epoklar arasÄ±nda kosinÃ¼s dalgasÄ± ÅŸeklinde LR'yi azaltÄ±r.
        *   `ReduceLROnPlateau`: 40. epoktan sonra doÄŸrulama kaybÄ± plato yaptÄ±ÄŸÄ±nda LR'yi 0.1 faktÃ¶rÃ¼ ile azaltÄ±r.
    *   **LR KÄ±sÄ±tlamalarÄ±:**
        *   40-50. epoklar arasÄ±nda minimum LR `1e-3`'Ã¼n altÄ±na dÃ¼ÅŸmez.
        *   50. epoktan sonra minimum LR `1e-4`'Ã¼n altÄ±na dÃ¼ÅŸmez.
        *   **Dinamik LR AyarlamasÄ±:** Her epok sonunda doÄŸrulama metriklerine gÃ¶re LR'yi otomatik olarak ayarlar.
*   **Otomatik KarÄ±ÅŸÄ±k Hassasiyet (AMP):** `torch.amp.autocast` ve `torch.cuda.amp.GradScaler` ile eÄŸitim hÄ±zÄ± ve bellek verimliliÄŸi artÄ±rÄ±lÄ±r.
*   **Gradiyent Biriktirme (Gradient Accumulation):** Efektif batch boyutunu artÄ±rmak iÃ§in gradyanlar birden fazla adÄ±mda biriktirilir.
*   **Rastgele AÄŸÄ±rlÄ±k OrtalamasÄ± (Stochastic Weight Averaging - SWA):** `AveragedModel` kullanÄ±larak eÄŸitimin son aÅŸamalarÄ±nda model aÄŸÄ±rlÄ±klarÄ±nÄ±n ortalamasÄ± alÄ±narak genelleme yeteneÄŸi artÄ±rÄ±lÄ±r. (YapÄ±landÄ±rmaya gÃ¶re etkinleÅŸtirilebilir/devre dÄ±ÅŸÄ± bÄ±rakÄ±labilir.)
*   **Mixup:** Veri artÄ±rma tekniÄŸi olarak Mixup kullanÄ±lÄ±r. Girdi gÃ¶rÃ¼ntÃ¼leri ve etiketleri lineer olarak karÄ±ÅŸtÄ±rÄ±lÄ±r. Mixup alfa deÄŸeri eÄŸitimin erken aÅŸamalarÄ±nda lineer olarak azaltÄ±lÄ±r (0. epoktan 15. epoka kadar).
*   **Dinamik DÃ¼zenlileÅŸtirme:** Modelin aÅŸÄ±rÄ± veya az Ã¶ÄŸrenmesine gÃ¶re dropout oranlarÄ±nÄ± dinamik olarak ayarlayan bir mekanizma.
*   **SÄ±caklÄ±k Ã–lÃ§eklendirme (Temperature Scaling):** Modelin tahmin gÃ¼venilirliÄŸini kalibre etmek iÃ§in kullanÄ±lÄ±r.
*   **Geri Alma MekanizmasÄ± (Rollback):** Performans dÃ¼ÅŸÃ¼ÅŸlerinde modelin Ã¶nceki iyi aÄŸÄ±rlÄ±klara dÃ¶nmesini saÄŸlar.
*   **SÄ±nÄ±f Dengeli AÄŸÄ±rlÄ±klar:** Veri setindeki sÄ±nÄ±f dengesizliÄŸini ele almak iÃ§in `get_class_balanced_weights` ile aÄŸÄ±rlÄ±klar hesaplanÄ±r ve `WeightedRandomSampler` ile DataLoader dengelenir.
*   **SÄ±nÄ±f CezasÄ± (ClassPenalty):** EÄŸitim ve doÄŸrulama doÄŸruluÄŸu arasÄ±ndaki farka gÃ¶re belirli sÄ±nÄ±flarÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ± dinamik olarak ayarlar.
*   **Erken Durdurma:** Belirli bir epok boyunca performans artÄ±ÅŸÄ± olmazsa eÄŸitimi sonlandÄ±rÄ±r.
*   **Dinamik Veri ArtÄ±rma:** Epoklara gÃ¶re deÄŸiÅŸen ÅŸiddetlerde veri artÄ±rma stratejileri uygulanÄ±r.

## GPU Performans OptimizasyonlarÄ±

Proje, GPU performansÄ±nÄ± en Ã¼st dÃ¼zeye Ã§Ä±karmak ve eÄŸitim sÃ¼recini hÄ±zlandÄ±rmak iÃ§in Ã§eÅŸitli modern teknolojiler ve teknikler kullanÄ±r:

*   **Otomatik KarÄ±ÅŸÄ±k Hassasiyet (Automatic Mixed Precision - AMP):**
    *   `torch.amp.autocast` ve `torch.cuda.amp.GradScaler` kullanarak hem hÄ±z hem de bellek verimliliÄŸi iÃ§in Float32 ve Float16 veri tiplerini dinamik olarak karÄ±ÅŸtÄ±rÄ±r. Uyumlu GPU'larda (Tensor Core gibi) daha hÄ±zlÄ± matematik iÅŸlemleri saÄŸlar ve bellek kullanÄ±mÄ±nÄ± optimize eder.

*   **Gradiyent Biriktirme (Gradient Accumulation):**
    *   `GRADIENT_ACCUMULATION_STEPS` ile belirtilen adÄ±mlarla gradyanlarÄ± biriktirerek daha bÃ¼yÃ¼k bir "efektif batch boyutu" simÃ¼le eder. Bu, GPU belleÄŸi kÄ±sÄ±tlÄ±yken bile bÃ¼yÃ¼k batch boyutlarÄ±nÄ±n faydalarÄ±ndan yararlanmayÄ± saÄŸlar.

*   **`DataLoader` OptimizasyonlarÄ±:**
    *   **`num_workers`:** Veri yÃ¼klemesini ayrÄ± alt iÅŸlemlere daÄŸÄ±tarak CPU'nun veri hazÄ±rlÄ±ÄŸÄ±na, GPU'nun ise model eÄŸitimine odaklanabilmesini saÄŸlar. (Windows iÃ§in kÄ±sÄ±tlamalar nedeniyle otomatik olarak 0'a ayarlanÄ±r).
    *   **`pin_memory=True`:** Verinin GPU'ya kopyalanmasÄ±nÄ± hÄ±zlandÄ±rmak iÃ§in CPU belleÄŸinde sabitlenmiÅŸ bir alana yÃ¼klenmesini saÄŸlar.
    *   **`persistent_workers=True`:** `DataLoader` alt iÅŸlemlerinin epochlar arasÄ±nda yeniden baÅŸlatÄ±lmasÄ±nÄ± engelleyerek ek yÃ¼kÃ¼ azaltÄ±r ve veri yÃ¼kleme sÃ¼resini kÄ±saltÄ±r. (Windows iÃ§in kÄ±sÄ±tlamalar nedeniyle devre dÄ±ÅŸÄ± bÄ±rakÄ±labilir).

*   **GPU Bellek YÃ¶netimi:**
    *   **`torch.cuda.empty_cache()`:** GPU Ã¼zerindeki Ã¶nbelleÄŸi boÅŸaltarak bellek fragmentasyonunu ve gereksiz bellek kullanÄ±mÄ±nÄ± azaltÄ±r.
    *   **`gc.collect()`:** Python'Ä±n Ã§Ã¶p toplayÄ±cÄ±sÄ±nÄ± manuel olarak tetikleyerek GPU belleÄŸinin daha verimli serbest bÄ±rakÄ±lmasÄ±na yardÄ±mcÄ± olur.
    *   **`set_to_none=True` ile `optim.zero_grad()`:** Gradyan tensÃ¶rlerini doÄŸrudan sÄ±fÄ±rlamak yerine `None` olarak ayarlayarak bellek tahsisini optimize eder.

*   **`ConvBlock` Ä°yileÅŸtirmeleri (DolaylÄ± Etki):**
    *   **`GroupNorm`:** Daha kÃ¼Ã§Ã¼k batch boyutlarÄ±nda stabilite saÄŸlar ve bazÄ± GPU mimarilerinde performansÄ± artÄ±rabilir.
    *   **`kernel_size=3`:** Daha az hesaplama gerektirir ve daha derin aÄŸlarÄ±n daha verimli Ã§alÄ±ÅŸmasÄ±na olanak tanÄ±r.
    *   **`SiLU()` aktivasyon fonksiyonu:** DiÄŸer bazÄ± aktivasyonlara gÃ¶re daha dÃ¼ÅŸÃ¼k hesaplama maliyetine sahip olabilir.
    *   **`nn.Dropout2d` (Spatial Dropout):** Uzamsal baÄŸÄ±ntÄ±larÄ± koruyarak daha etkili bir dÃ¼zenlileÅŸtirme ve genelleme saÄŸlar, bu da daha az epokta iyi sonuÃ§lar elde etmeye yardÄ±mcÄ± olabilir.

## Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme (dataset/custom_dataset.py)

Veri seti iÅŸlemleri iÃ§in Ã¶zel bir PyTorch `Dataset` sÄ±nÄ±fÄ± kullanÄ±lmÄ±ÅŸtÄ±r:

*   **`CustomTumorDataset`:** `.npy` formatÄ±ndaki Ã¶n iÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼lerden veri yÃ¼kler.
*   **Ã–nbellekleme:** SÄ±k eriÅŸilen veriler iÃ§in bellek Ã¶nbelleklemesi kullanÄ±r.
*   **GÃ¶rÃ¼ntÃ¼ ArtÄ±rma ve DÃ¶nÃ¼ÅŸÃ¼mler:** `Albumentations` kÃ¼tÃ¼phanesi ile Ã§eÅŸitli rastgele dÃ¶nÃ¼ÅŸÃ¼mler uygulanÄ±r. Bu dÃ¶nÃ¼ÅŸÃ¼mler eÄŸitim epoklarÄ±na gÃ¶re dinamik olarak ayarlanÄ±r:
    *   **Erken EÄŸitim FazÄ± (0-10 Epok):** Modelin daha Ã§eÅŸitli verilere maruz kalmasÄ± iÃ§in daha yÃ¼ksek ÅŸiddetli artÄ±rmalar uygulanÄ±r.
        *   `A.Rotate(limit=5, p=0.3)`: GÃ¶rÃ¼ntÃ¼leri 5 dereceye kadar dÃ¶ndÃ¼rme olasÄ±lÄ±ÄŸÄ± 0.3.
        *   `A.RandomRotate90(p=0.1)`: GÃ¶rÃ¼ntÃ¼leri 90 derece dÃ¶ndÃ¼rme olasÄ±lÄ±ÄŸÄ± 0.1.
        *   `A.OneOf([A.HorizontalFlip(p=1.0), A.VerticalFlip(p=1.0)], p=0.3)`: Yatay veya dikey Ã§evirme olasÄ±lÄ±ÄŸÄ± 0.3.
        *   `A.Affine(translate_percent={'x': 0.02, 'y': 0.02}, scale={'x': (0.96, 1.04), 'y': (0.96, 1.04)}, p=0.3)`: Hafif Ã§eviri ve Ã¶lÃ§ekleme olasÄ±lÄ±ÄŸÄ± 0.3.
        *   `A.RandomBrightnessContrast(brightness_limit=0.08, contrast_limit=0.08, p=0.3)`: ParlaklÄ±k ve kontrast ayarlama olasÄ±lÄ±ÄŸÄ± 0.3.
        *   `A.GaussNoise(std_range=(0.005, 0.015), mean_range=(0.0, 0.0), p=0.1)`: Gauss gÃ¼rÃ¼ltÃ¼sÃ¼ ekleme olasÄ±lÄ±ÄŸÄ± 0.1.
    *   **Orta EÄŸitim FazÄ± (10-40 Epok):** ArtÄ±rmalarÄ±n ÅŸiddeti ve olasÄ±lÄ±klarÄ± azaltÄ±larak modelin daha oturmuÅŸ Ã¶ÄŸrenmesine olanak tanÄ±nÄ±r.
        *   `A.Rotate(limit=3, p=0.1)`
        *   `A.RandomRotate90(p=0.05)`
        *   `A.OneOf([A.HorizontalFlip(p=1.0), A.VerticalFlip(p=1.0)], p=0.1)`
        *   `A.Affine(translate_percent={'x': 0.005, 'y': 0.005}, scale={'x': (0.99, 1.01), 'y': (0.99, 1.01)}, p=0.1)`
        *   `A.RandomBrightnessContrast(brightness_limit=0.02, contrast_limit=0.02, p=0.1)`
        *   `A.GaussNoise(std_range=(0.003, 0.008), mean_range=(0.0, 0.0), p=0.05)`
    *   **Son EÄŸitim FazÄ± (40+ Epok):** Minimal artÄ±rmalar uygulanÄ±r, genellikle sadece temel dÃ¶nÃ¼ÅŸÃ¼mlerle modelin ince ayar yapmasÄ±na izin verilir.
        *   `A.HorizontalFlip(p=1.0)`: GÃ¶rÃ¼ntÃ¼leri her zaman yatay Ã§evir.
    *   **Sabit DÃ¶nÃ¼ÅŸÃ¼mler:** TÃ¼m artÄ±rma aÅŸamalarÄ±ndan sonra uygulanan sabit dÃ¶nÃ¼ÅŸÃ¼mler:
        *   `A.Resize(224, 224)`: GÃ¶rÃ¼ntÃ¼leri hedef boyut olan 224x224 piksele yeniden boyutlandÄ±rÄ±r.
        *   `A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))`: Piksel deÄŸerlerini [-1, 1] aralÄ±ÄŸÄ±na normalize eder.
        *   `ToTensorV2()`: Son adÄ±mda NumPy dizilerini PyTorch tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

## YardÄ±mcÄ± Betikler

*   **`test.py`:** EÄŸitilmiÅŸ modelin test veri seti Ã¼zerindeki performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r. Hem en iyi modeli hem de SWA modelini test edebilir.
*   **`losses.py`:** Gelecekte Ã¶zel kayÄ±p fonksiyonlarÄ± eklemek iÃ§in yer tutucu.
*   **Renk Kodlu Konsol Ã‡Ä±ktÄ±sÄ±:** Terminal Ã§Ä±ktÄ±larÄ±nÄ± renklendirmek iÃ§in Ã¶zel ANSI kaÃ§Ä±ÅŸ kodlarÄ± kullanÄ±lÄ±r.
*   **Windows iÃ§in Ã‡oklu Ä°ÅŸlem DesteÄŸi:** `multiprocessing` modÃ¼lÃ¼, Windows iÅŸletim sistemlerinde uyumluluÄŸu saÄŸlamak iÃ§in yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.

## BaÅŸlangÄ±Ã§

Projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼klemeniz ve ardÄ±ndan `models/train.py` betiÄŸini Ã§alÄ±ÅŸtÄ±rmanÄ±z yeterlidir.


# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin 
pip install -r requirements.txt

# Modeli eÄŸitin
python models/train.py

# Modeli test edin (eÄŸitimden sonra)
python test.py
```
~~python test.py
ğŸ” Veri yÃ¼kleniyor...
ğŸ¤– EÄŸitilmiÅŸ model models\full_vgg_custom.pt yÃ¼klendi.
ğŸ” Tahminler yapÄ±lÄ±yor...

ğŸ¯ 912 test Ã¶rneÄŸinden 891 tanesi doÄŸru tahmin edildi.
âœ… DoÄŸruluk OranÄ±: 97.70%
ğŸ“ˆ KarÄ±ÅŸÄ±klÄ±k Matrisi Ã§iziliyor...

SÄ±nÄ±flandÄ±rma Raporu:
              precision    recall  f1-score   support

    0_glioma       0.99      0.98      0.99       302
     1_menin       0.98      0.95      0.97       302
     2_tumor       0.96      1.00      0.98       308

    accuracy                           0.98       912
   macro avg       0.98      0.98      0.98       912
weighted avg       0.98      0.98      0.98       912
---

